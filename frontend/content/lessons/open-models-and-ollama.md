# Open Models & Ollama

**What you'll learn**
- Open-source model options
- Running locally with Ollama
- Trade-offs vs hosted APIs

---

## Short summary

Open models allow running inference locally or on your infra. Ollama and vLLM provide easy hosting for some open models.

---

## Key concepts

- **Latency & cost** — Local inference requires GPUs for large models.
- **Model selection** — Choose model size by use-case.
- **Compatibility** — Tokenizer and API differences matter.

---

## Exercise

- Run a small open model locally (CPU-friendly) and compare prompts.
- Document differences in outputs vs hosted models.

---

## Further reading

- Ollama docs
- Hugging Face model hub
